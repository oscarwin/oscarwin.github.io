---
title: 基于Redis实现延时任务
date: 2019-12-10 10:00:00
tags:
    - redis
    - 延时消息
categories: 技术博文
comments:  true
---

## 背景介绍

最近业务上有个需求，背景如下：有一个养殖类游戏，通过给养的宠物喂食来升级，一次喂食后，宠物需要花4个小时吃完。现在有个新需求，可以使用道具卡来丰富玩法。道具卡有两种，一种是加速卡，一种是自动喂食卡。加速卡会使吃食的时间缩短两个小时，自动喂食卡可以在宠物吃完当前喂食的狗粮后系统帮助其自动喂食一次。

业务需求里的自动喂食就是一种典型的延时任务。延时任务是指需要在指定的未来的某个时间点自动触发。与之类似的场景还有：

- 活动结束前 2 小时给用户推送提醒消息；
- 优惠券过期前 2 小时给用户推送提醒消息；
- 秒杀时，下单后 10 分钟内未付款就自动取消订单；<!-- more -->

## 业界解决方案

下面我们讨论一下，对于这一类问题通常的解决方案有哪些。

### 扫表

对于延时任务，常见的方案就是扫表。扫表是指用一个定时任务，每隔一段时间扫描数据库的整张数据表，判断每个任务是否达到触发的条件。如果达到条件就执行相应的任务。

扫表实现起来比较简单，而且数据本身存在 DB 里，因此也不用担心任务数据会丢失，失败的任务可以下次扫描时再重入。使用扫表这种方案时需要考虑以下几点：

1. 扫描全表对数据库压力较大，如果可以接受主从同步之前可能存在的延时，那么可以选择扫从库，减轻主库的压力；
2. 如果进行了分表，那么可以启动多个线程分表扫描多张表，可以加快扫描的速度，减少因为扫表任务带来的时间误差；

但是扫表方法可能存在以下问题：

- 扫表一整张表需要一段时间，会造成任务的触发有延时。比如有一张表，该表的第一万行到达了任务时间，但是从一行开始扫表，每次扫 100 行，扫描到 100 次才能扫到第一万行的任务，扫到一万行时该任务已经比理论上应该启动的时间晚了一些；
- 扫表不可能太频繁，因为太频繁会对数据库造成太大压力，每隔一段较长的时间才能再扫一遍，这个时间间隔一般至少在一分钟以上。这也会造成任务延时；
- 扫表扫的是从库，而主从同步存在延时。特别是当大事务出现时，会导致几分钟甚至几小时的延时；
- 扫表的方法很笨重，每次扫描一整张表而实际需要触发的任务可能没几个，资源利用很低下；

扫表最大的问题就是会有延迟，不能再指定的时间里触发，对于时效性要求不高，数据量不大的场景下，可以使用该方案。但是文章开始提到的业务场景对时效性要求比较高，这种方案不能满足需求。

### 延时消息队列

延时消息就是消息投递到消息中间件以后，可以指定延时一段时间后才会被消费。

<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggg76546toj317q05674e.jpg" style="zoom:50%;" />

一些常用的消息队列组件里，RocketMQ 支持延时消息，但是 Kafka 是不支持延时消息的。开源版本的 RocketMQ 只支持特定时间长度的延迟，不支持任意时间长度的延迟，支持的时间长度延时有以下几个梯度： `1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h`。

这个方案，开发成本也很小，不过需要使用的中间件能支持延时消息。而且该方案也存在一个问题是消息重新投递所选的中间件是否能够支持。

#### RocketMQ 的延时消息实现方案

---- 待补充

### 时间片轮询

用环形队列做成时间片，环形队列的每个格子里维护一个链表。每个时刻有一个当前指针指向环形队列某个格子，定时器每超时一次，就把当前指针指向下环形队列的下一个格子。然后处理这个格子保存的链表里的任务。如果只是这样维护，如果要做到秒级的粒度，时间长度最长一天，那么这个环形队列就会非常大。因此，需要改进了一下，当存在任务进入队列时，就用时间长度除以环形队列的长度，记为圈数。这样每次遍历到该元素时，将圈数减一，如果减一后为0就执行改任务，否者不执行。

对于时间跨度非常大的场景，如果使用这种方法会导致链表上的元素非常多，遍历链表的开销也不小，甚至在一个时间片内遍历不完。因此，又有了进一步的改进，将时间片分为不同粒度的。比如，粒度为小时的时间轮，粒度为分钟的时间轮，粒度为秒钟的时间轮。小时里的时间轮达到触发的条件后会放到分钟的时间轮里，分钟的时间轮到达触发的条件后会放到秒的时间轮里。(图片来自网络，侵删)
<img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gggyp2w7elj30o30duq3i.jpg" style="zoom:100%;" />
该方案时间片存放在内存，因此轮询起来效率非常高，也可以根据不同的粒度调整时间片，因此也非常灵活。但是该方案需要自己实现持久化与高可用，以及对储存的管理，如果没有现成的轮子开发耗时会比较长。

关于时间轮的具体实现可以参考：[多级时间轮定时器]([http://www.langdebuqing.com/algorithm%20notebook/%E5%A4%9A%E7%BA%A7%E6%97%B6%E9%97%B4%E8%BD%AE%E5%AE%9A%E6%97%B6%E5%99%A8.html](http://www.langdebuqing.com/algorithm notebook/多级时间轮定时器.html))

### Redis的ZSET实现

Redis实现延时任务，是通过其数据结构ZSET来实现的。ZSET会储存一个score和一个value，可以将value按照score进行排序，而SET是无序的。

延时任务的实现分为以下几步来实现：

(1) 将任务的执行时间作为score，要执行的任务数据作为value，存放在zset中；
(2) 用一个进程定时查询zset的score分数最小的元素，可以用ZRANGEBYSCORE key -inf +inf limit 0 1 withscores命令来实现;
(3) 如果最小的分数小于等于当前时间戳，就将该任务取出来执行，否则休眠一段时间后再查询

redis的ZSET是通过跳跃表来实现的，复杂度为O(logN)，N是存放在ZSET中元素的个数。用redis来实现可以依赖于redis自身的持久化来实现持久化，redis的集群来支持高并发和高可用。因此开发成本很小，可以做到很实时。

## 具体实现

扫表的方法延时太高不能满足实时的需求，团队目前使用的消息队列还不支持延时消息队列，时间轮的方法开发起来很耗时，因此最终选择了Redis来实现。

前面介绍了Redis实现延时任务的原理，为了实现更高的并发还需要在原理的基础上进行设计。接下来将详细阐述具体的实现。架构设计图如下：


![](https://user-gold-cdn.xitu.io/2019/4/12/16a11e7e3a513253?w=1704&h=522&f=png&s=98317)

说明：

- 为了避免一个key存储在数据量变多以后，首先会导致查询速度变慢，因为其时间复杂度为O(logN)，其次如果在同一个时间点有多个任务时，一个key会分发不过来，造成拥堵。因此，我们将其设计为多个key来存储，通过uuid进行hash路由到对应的key中，如果任务量增长，我们可以快速扩容redis key的数量来抗住增长的数量；
- 建立与多个key相同的进程或者线程数，每个进程一个编号，分别对应一个key，不断轮询相应的key；
- 轮询key的进程我们将其称为event进程，event进程只查询出任务，但是不处理业务，将该任务写入到消息队列中。另外有work进行从消息队列取消息，然后执行业务。这样work进行可以分布式部署，event进行只需做分发，这样可以把并发做到非常高，即使同一时间有大量的任务，也能很小的延时内完成任务；
- 为了避免event进程单机部署，在机器宕机后导致无法取消息，redis储存的数据还会被积压。我们多机部署event进程，并使用zookeeper选主，只有leader主机上的进程才从redis取消息。leader主机宕机后，zookeeper会自动选择新的leader；
- 在实际的业务中，还依赖DB写入数据。延时任务产生是先修改DB然后再向redis写入数据，那么就存在DB更新成功，然后redis写失败的场景，这个时候首先是通过重试来减少redis写入失败的概率，如果重试任然不能成功，就发送一条消息给daemon进程进行异步补偿；

在延时任务的基础上，本次业务还有一个需求，就是延时任务如果还没有到达执行时间，那么该延时任务的时间是可以被更改的。为了实现这个需求，我们另外给每个用户维护一个ZSET，这个ZSET中存放该用户所有的延时任务。为了便于描述，我们将这个ZSET称为ZSET-USER。如果用户需要修改其延时任务，如果没有办法从整体的延时任务的ZSET中找到这个任务，而是即使能找到，也只能遍历这个ZSET，显然这种方法太慢，太耗资源。我们采取的方法是从ZSET-USER中取出这个用户的延时任务，然后修改score，最后重新ZADD到延时任务ZSET和ZSET-USER中，ZADD会覆盖原来的任务，而score则发生了更新。这样看来，这个通过Redis来实现还是很合适的。

### 待优化空间

以上的设计只是初版的设计，还存在许多可以优化的点。

1. 可以将任务具体的信息，单独用 Key-Value 结构存储，ZSET 里只存储任务 ID，这样可以减小 ZSET 的数量。
2. ZSET 数据量过大后，容易成为大 KEY，如果使用的 Redis 架构扩容时是同步迁移，那么大 KEY 迁移会导致 Redis 抖动。因此，最好有自动扩容的方案。
3. 简单的 HASH 负载均衡可能会导致 ZSET 里 KEY 的数量分布不均，第一是某些 ZSET 里任务延时比较长，消费的比较慢，因此堆积比较多，第二是自动扩容后，新扩容的 ZSET 里 KEY 数量比较少。为了因为这种情况，可以采用加权 HASH 进行重平衡，权重与 ZSET 中 KEY 的数量成反比。
4. 现在是一个进程负责一个 ZSET 的轮询，如果进程奔溃，会导致整个服务不可用，因此 Event 可以进行多机部署，通过分布式锁自动抢占。强占到 ZSET 的进程，如果进程健康，就每隔一段时间续期一次分布式锁的过期时间，这样一般情况下还是由某一个进程进行稳定消费。

## 有赞延迟队列设计

再完成了基于 Redis 的延时消息队列的实践后，后面了解到有赞也有一套基于 Redis 的延迟消息队列设计，架构图如下所示：

![](https://tech.youzan.com/content/images/2016/03/delay-queue.png)

- Job Pool用来存放所有Job的元信息。
- Delay Bucket是一组以时间为维度的有序队列，用来存放所有需要延迟的／已经被reserve的Job（这里只存放Job Id）。
- Timer负责实时扫描各个Bucket，并将delay时间大于等于当前时间的Job放入到对应的Ready Queue。
- Ready Queue存放处于Ready状态的Job（这里只存放Job Id），以供消费程序消费。



整体来看有赞的设计方案与我的实现方案基本一致，不过有赞的方案将具体的 Job 使用了 Job Pool 来存储，Delay Bucket 里只存储 Job ID，这样可以减小 ZSET 的大小。有赞方案中的 Delay Bucket 类似本方案中的 ZSET 存储，实际上 Delay Bucket 也是基于 Redis 的 ZSET 来存储的。有赞方案中的 Timer 则类似本方案中的 Event，将就绪的任务从 Delay Bucket 迁移到 Ready Queue 中，Ready Queue 就是一个普通的消息队列。

## 总结

本篇文章，借着业务需求的背景首先探讨了延时任务的业界常见的实现方案，实际上关于延时队列的实践还有更多的方案。详细阐述了通过redis来实现延时任务方法，并分析了高并发，高可用的设计思路。数据量比较小时，可以直接通过扫表的方式来实现。如果使用的是 RocketMQ，且能够接受固定 Level 的延时，那么可以直接使用消息中间件来完成。如果以上都不满足则需要自己开发，或者基于已有的消息队列进行改造。

## 参考

1. [有赞延迟队列设计](https://tech.youzan.com/queuing_delay/)
2. [多级时间轮定时器]([http://www.langdebuqing.com/algorithm%20notebook/%E5%A4%9A%E7%BA%A7%E6%97%B6%E9%97%B4%E8%BD%AE%E5%AE%9A%E6%97%B6%E5%99%A8.html](http://www.langdebuqing.com/algorithm notebook/多级时间轮定时器.html))
3. [定时器的几种实现方式](https://www.cnkirito.moe/timer/)



> 本文作者：ltengpeter@gmail.com
> 本文链接：https://oscarwin.github.io/2019/12/10/create_a_delay_queue_by_redis
> 版权声明：本博客所有文章除特别声明外，均采用[by-nc-sa 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/)许可协议。转载请注明原文出处！

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1ggfz9k5x0nj31mo0jatai.jpg)